# PII Masking Evaluation Framework

**Mistral Take-Home Project** 

A comprehensive framework for evaluating and comparing PII (Personally Identifiable Information) masking approaches using various LLM techniques including prompting and fine-tuning.

### Live Demo
**[Try it live](https://huggingface.co/spaces/SoelMgd/pii_masking)**

## Project Overview

This project evaluates whether **prompt engineering** can compete with **fine-tuned models** and token-classiciation approach for PII masking tasks, enabling data-driven decisions about the optimal approach.

### Dataset: AI4Privacy PII
- **48k english and 64k french examples**
- **54 PII classes** covering comprehensive privacy scenarios
- **Human-validated** synthetic data with no privacy violations

## Project Structure

```
pii-masking-200k/
â”œâ”€â”€ src/pii_masking/           # Core framework package
â”‚   â”œâ”€â”€ __init__.py               # Package exports
â”‚   â”œâ”€â”€ data_loader.py            # Dataset loading utilities
â”‚   â”œâ”€â”€ text_processing.py       # Text processing and reconstruction
â”‚   â”œâ”€â”€ custom_evaluator.py      # Evaluation with exact position matching
â”‚   â””â”€â”€ base_model.py             # Abstract model interfaces
â”‚
â”œâ”€â”€ experiments/               # Research experiments & analysis
â”‚   â”œâ”€â”€ mistral_prompting_baseline.py    # Mistral API prompting
â”‚   â”œâ”€â”€ mistral_finetuning.py           # Mistral fine-tuning experiments
â”‚   â”œâ”€â”€ bert_token_classification/      # BERT experiments
â”‚   â”‚   â”œâ”€â”€ bert_finetuning_kaggle.ipynb # BERT training notebook
â”‚   â”‚   â””â”€â”€ eval.py                      # BERT evaluation script
â”‚   â”œâ”€â”€ visualization/               # Results visualization
â”‚   â”‚   â””â”€â”€ results_visualization.py    # Performance charts & comparisons
â”‚   â””â”€â”€ evaluation_comparison.py        # Cross-method comparison
â”‚
â”œâ”€â”€ space/                     # Production deployment (HuggingFace Space)
â”‚   â”œâ”€â”€ app.py                    # FastAPI production server with async/sync architecture
â”‚   â”œâ”€â”€ services/                 # Smart inference services (refactored)
â”‚   â”‚   â”œâ”€â”€ base_service.py       # Unified async/sync interface
â”‚   â”‚   â”œâ”€â”€ mistral_prompting.py  # Mistral API service (native async)
â”‚   â”‚   â”œâ”€â”€ bert_classif.py       # BERT inference service (sync + thread pool)
â”‚   â”‚   â””â”€â”€ ocr_service.py        # PDF OCR processing (Mistral OCR)
â”‚   â”œâ”€â”€ static/                   # Frontend assets
â”‚   â”‚   â””â”€â”€ index.html            # Web interface with PDF drag-and-drop
â”‚   â”œâ”€â”€ src/                      # Shared core package (symlink)
â”‚   â”œâ”€â”€ pyproject.toml           # Space dependencies
â”‚   â”œâ”€â”€ Dockerfile               # Container configuration
â”‚   â””â”€â”€ README.md                # Space deployment guide
â”‚
â”œâ”€â”€ models/                    # Trained model artifacts
â”‚   â”œâ”€â”€ bert_classic_token_classif/     # BERT Classic fine-tuned model
â”‚   â””â”€â”€ bert_token_classif/             # DistilBERT fine-tuned model
â”‚
â”œâ”€â”€ data/                      # Dataset files (AI4Privacy PII-200k)
â”œâ”€â”€ configs/                  # Configuration files
â”œâ”€â”€ results/                   # Experiment results & visualizations
â”‚
â”œâ”€â”€ presentation.tex           # LaTeX presentation (Mistral AI take-home)
â”œâ”€â”€ pyproject.toml             # Project configuration
â”œâ”€â”€ .env.example               # Environment variables template
â””â”€â”€ README.md                  # This file
```

### Repository Components

#### **Research & Development** (`src/` + `experiments/`)
- **Core framework**: Reusable PII detection and evaluation components
- **Experiments**: Comparative studies between prompting, fine-tuning, and token classification
- **Benchmarking**: Systematic evaluation on AI4Privacy PII dataset

#### **Production Deployment** (`space/`)
- **Web application**: FastAPI server with modern frontend
- **Multi-method support**: Mistral prompting, fine-tuning, and BERT classification
- **Advanced features**: PDF processing with OCR, entity selection, drag-and-drop UI
- **HuggingFace Space**: Ready for production deployment at scale

## Quick Start

### 1. Installation (avec UV - recommandÃ©)

```bash
# Clone and navigate to project
cd pii-masking-200k

# Synchronize dependencies with uv
uv sync

# Copy environment template
cp .env.example .env
# Edit .env with your MISTRAL_API_KEY
```

### 2. Run Baseline Experiment

```bash
cd experiments

uv run mistral_baseline.py --samples 500 --few-shot --output ../results/mistral_baseline_full.json
```


## Evaluation Methodology

### Strict Position Matching 
- **Exact position requirement**: Entities must match type AND exact character positions
- **No partial credit**: Any position mismatch = error (prevents text rewriting issues)
- **Per-class metrics**: F1-score calculated for each PII type individually

### Standardized Output Format
All experiments return `EvaluationResult` objects with:
- **Global metrics**: Precision, Recall, F1-Score, Exact Match Accuracy
- **Per-class metrics**: Performance breakdown by PII type
- **Detailed results**: Individual predictions for error analysis

## Available Experiments

### 1. Mistral API Prompting (`mistral_prompting_baseline.py`)
- **Approach**: Sophisticated prompt engineering + few-shot learning
- **Models**: mistral-large-latest, fine-tuned models
- **Features**: Rate limiting, retry logic, batch processing, configurable prompts
- **Languages**: English & french

```bash
cd experiments
uv run mistral_prompting_baseline.py \
    --samples 100 \
    --few-shot \
    --num-few-shot 3 \
    --temperature 0.1 \
    --output ../results/mistral_prompting.json
```

### 2. Mistral Fine-tuning (`mistral_finetuning.py`)
- **Approach**: Fine-tuned Mistral models via API
- **Features**: Custom model training, specialized PII detection
- **Performance**: Enhanced accuracy on domain-specific patterns
- **Languages**: English & french

### 3. BERT Token Classification (`bert_test.py`)
- **Approach**: Fine-tuned BERT for token-level PII detection
- **Models**: DistilBERT, BERT-base optimized for CPU inference
- **Features**: Fast inference, offline deployment capability
- **Languages**: English only

### 4. Cross-Method Evaluation (`evaluation_comparison.py`)
- **Comprehensive comparison** across all three approaches
- **Performance metrics**: Precision, Recall, F1-Score by entity type
- **Cost analysis**: API costs vs. fine-tuning investment


## Production Deployment (HuggingFace Space)

The `space/` directory contains a complete production-ready web application deployed on HuggingFace Spaces.

### Features
- **Multi-method PII detection**: Mistral prompting, fine-tuning, and BERT classification
- **PDF processing**: Drag-and-drop PDF upload with Mistral OCR integration
- **Entity selection**: Granular control over which PII types to mask
- **Smart async/sync architecture**: Optimal performance for different service types
- **Concurrent processing**: Multiple users served simultaneously without blocking
- **Optimized inference**: CPU-optimized BERT, batched Mistral API calls
- **Modern UI**: Responsive interface with real-time feedback

### Quick Deploy
```bash
cd space
uv sync
uv run app.py
# Visit http://localhost:7860
```


### Architecture
- **Backend**: FastAPI with smart async/sync processing architecture
- **Services**: Unified interface with automatic routing (async for Mistral, thread pool for BERT)
- **Frontend**: Modern HTML/CSS/JS with drag-and-drop
- **Models**: HuggingFace Hub integration for BERT, Mistral API
- **OCR**: Mistral OCR service for PDF text extraction
- **Concurrency**: Non-blocking event loop, multiple users served simultaneously

## Technical Architecture: Smart Async/Sync Pattern

### The Challenge
Different AI services have different computational characteristics:
- **BERT**: CPU-bound operations (PyTorch inference) - naturally synchronous
- **Mistral API**: I/O-bound operations (network calls) - naturally asynchronous
- **FastAPI**: Async framework requiring non-blocking operations

### Our Solution: Unified Interface with Automatic Routing

```python
class BasePIIInferenceService(ABC):
    async def predict(self, text: str) -> PIIPrediction:
        # Try native async first (for API-based models)
        try:
            return await self.predict_async_native(text)
        except NotImplementedError:
            # Fall back to sync in thread pool (for local models)
            return await asyncio.to_thread(self.predict_sync, text)

# BERT Service - implements sync pattern
class BERTInferenceService(BasePIIInferenceService):
    def predict_sync(self, text: str) -> PIIPrediction:
        # Synchronous PyTorch operations
        outputs = self.model(**inputs)
        return self.process_predictions(outputs)

# Mistral Service - implements async pattern  
class MistralPromptingService(BasePIIInferenceService):
    async def predict_async_native(self, text: str) -> PIIPrediction:
        # Native async API calls
        response = await self.client.chat(...)
        return self.process_json_response(response)
```

### Performance Benefits
- **2.3x faster** under concurrent load
- **Non-blocking**: BERT inference doesn't block Mistral API calls
- **Scalable**: Multiple users served simultaneously
- **Maintainable**: Clean separation of sync/async concerns

### Adding New Experiments

1. **Inherit from base classes**:
   ```python
   from pii_masking.base_model import PromptBasedModel
   ```

2. **Use standard evaluation**:
   ```python
   from pii_masking import PIIEvaluator, EvaluationResult
   
   evaluator = PIIEvaluator(strict_matching=True)
   result = evaluator.evaluate_dataset(examples, predictions, ...)
   ```

3. **Follow naming convention**: `{model_type}_{approach}.py`

## ðŸ“‹ Configuration

### Environment Variables
```bash
MISTRAL_API_KEY=your_api_key_here
MISTRAL_MODEL=mistral-large-latest
LOG_LEVEL=INFO
```



## Results Analysis

### JSON Output Format
```json
{
  "experiment_name": "mistral_baseline",
  "model_name": "mistral-large-latest",
  "metrics": {
    "precision": 0.847,
    "recall": 0.793,
    "f1_score": 0.819,
    "exact_match_accuracy": 0.340
  },
  "per_class_metrics": {
    "FIRSTNAME": {"precision": 0.95, "recall": 0.92, "f1_score": 0.93},
    "EMAIL": {"precision": 0.88, "recall": 0.85, "f1_score": 0.86}
  }
}
```

### Visualization 
- Performance comparison charts
- Error analysis by PII type
- Position accuracy heatmaps

## Business Impact

### Privacy Compliance
- **GDPR/HIPAA** automated compliance
- **Real-time** PII detection and masking
- **Multi-language** support for global deployment

### Cost-Benefit Analysis
- **API Approach**: ~$0.05-0.15 per prediction, immediate deployment
- **Fine-tuning**: ~$50-100 initial cost, 2-3 days development
- **Break-even**: ~1000 predictions/month


## Contributing

This is a take-home project, but the framework is designed for extensibility:

1. **Add new models**: Inherit from `BasePIIModel`
2. **Extend evaluation**: Modify `PIIEvaluator` for new metrics
3. **New datasets**: Extend `PIIDataLoader` for different formats


**Ready for Production Deployment!**
