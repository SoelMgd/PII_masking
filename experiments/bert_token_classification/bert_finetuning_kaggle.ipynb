{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T20:10:01.440718Z",
     "iopub.status.busy": "2025-09-02T20:10:01.440450Z",
     "iopub.status.idle": "2025-09-02T20:10:33.085470Z",
     "shell.execute_reply": "2025-09-02T20:10:33.084852Z",
     "shell.execute_reply.started": "2025-09-02T20:10:01.440697Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".*tokenizer.*deprecated.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*FutureWarning.*\")\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "import logging\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainerCallback\n",
    ")\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sys\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout),\n",
    "    ],\n",
    "    force=True\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import transformers\n",
    "transformers.logging.set_verbosity_info()\n",
    "transformers.logging.enable_default_handler()\n",
    "transformers.logging.enable_explicit_format()\n",
    "\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "sys.stdout.flush()\n",
    "sys.stderr.flush()\n",
    "\n",
    "DATA_PATH = \"/kaggle/input/bert-token-classif\"\n",
    "OUTPUT_PATH = \"/kaggle/working\"\n",
    "\n",
    "\n",
    "print(f\"Data path: {DATA_PATH}\")\n",
    "print(f\"Output path: {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T20:10:33.086944Z",
     "iopub.status.busy": "2025-09-02T20:10:33.086451Z",
     "iopub.status.idle": "2025-09-02T20:10:33.093283Z",
     "shell.execute_reply": "2025-09-02T20:10:33.092542Z",
     "shell.execute_reply.started": "2025-09-02T20:10:33.086903Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"Training configuration parameters.\"\"\"\n",
    "    \n",
    "    model_name: str = \"distilbert-base-uncased\"\n",
    "    max_length: int = 512\n",
    "    \n",
    "    num_epochs: int = 3\n",
    "    batch_size: int = 16\n",
    "    learning_rate: float = 2e-5\n",
    "    weight_decay: float = 0.01\n",
    "    warmup_steps: int = 500\n",
    "    \n",
    "    train_split: float = 0.8\n",
    "    val_split: float = 0.1\n",
    "    test_split: float = 0.1\n",
    "    max_samples: Optional[int] = None\n",
    "    \n",
    "    save_model: bool = True\n",
    "    evaluate_on_test: bool = True\n",
    "\n",
    "config = TrainingConfig()\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "for key, value in config.__dict__.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T20:10:33.094506Z",
     "iopub.status.busy": "2025-09-02T20:10:33.094214Z",
     "iopub.status.idle": "2025-09-02T20:10:40.806879Z",
     "shell.execute_reply": "2025-09-02T20:10:40.806373Z",
     "shell.execute_reply.started": "2025-09-02T20:10:33.094479Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_processed_data(data_path: str) -> Tuple[List[Dict], Dict[str, int], Dict[int, str]]:\n",
    "    \"\"\"\n",
    "    Load processed dataset and label mappings.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (examples, label2id, id2label)\n",
    "    \"\"\"\n",
    "    with open(f\"{data_path}/processed_examples.json\", 'r', encoding='utf-8') as f:\n",
    "        examples = json.load(f)\n",
    "    \n",
    "    with open(f\"{data_path}/label_mappings.json\", 'r', encoding='utf-8') as f:\n",
    "        mappings = json.load(f)\n",
    "    \n",
    "    label2id = mappings['label2id']\n",
    "    id2label = mappings['id2label']\n",
    "    id2label = {int(k): v for k, v in id2label.items()}\n",
    "    \n",
    "    logger.info(f\"Loaded {len(examples)} examples\")\n",
    "    logger.info(f\"Number of labels: {len(label2id)}\")\n",
    "    logger.info(f\"Labels: {sorted(label2id.keys())}\")\n",
    "    \n",
    "    return examples, label2id, id2label\n",
    "\n",
    "examples, label2id, id2label = load_processed_data(DATA_PATH)\n",
    "\n",
    "if config.max_samples:\n",
    "    examples = examples[:config.max_samples]\n",
    "    logger.info(f\"Limited to {len(examples)} examples for testing\")\n",
    "\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"Total examples: {len(examples)}\")\n",
    "print(f\"Number of unique labels: {len(label2id)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T20:10:50.623523Z",
     "iopub.status.busy": "2025-09-02T20:10:50.622792Z",
     "iopub.status.idle": "2025-09-02T20:11:10.046159Z",
     "shell.execute_reply": "2025-09-02T20:11:10.045224Z",
     "shell.execute_reply.started": "2025-09-02T20:10:50.623497Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def split_data(examples: List[Dict], train_split: float, val_split: float, test_split: float):\n",
    "    \"\"\"Split data into train/val/test sets.\"\"\"\n",
    "    \n",
    "    assert abs(train_split + val_split + test_split - 1.0) < 1e-6, \"Splits must sum to 1.0\"\n",
    "    \n",
    "    n_total = len(examples)\n",
    "    n_train = int(n_total * train_split)\n",
    "    n_val = int(n_total * val_split)\n",
    "    \n",
    "    train_examples = examples[:n_train]\n",
    "    val_examples = examples[n_train:n_train + n_val]\n",
    "    test_examples = examples[n_train + n_val:]\n",
    "    \n",
    "    logger.info(f\"Data split: Train={len(train_examples)}, Val={len(val_examples)}, Test={len(test_examples)}\")\n",
    "    \n",
    "    return train_examples, val_examples, test_examples\n",
    "\n",
    "def create_dataset(examples: List[Dict], tokenizer, label2id: Dict[str, int], max_length: int):\n",
    "    \"\"\"\n",
    "    Create HuggingFace Dataset from processed examples.\n",
    "    \"\"\"\n",
    "    \n",
    "    def tokenize_and_align_labels(examples_batch):\n",
    "        \"\"\"Tokenize and align labels for a batch of examples.\"\"\"\n",
    "        \n",
    "        tokenized_inputs = tokenizer(\n",
    "            [ex['tokens'] for ex in examples_batch],\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=max_length,\n",
    "            is_split_into_words=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        labels = []\n",
    "        for i, example in enumerate(examples_batch):\n",
    "            word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "            label_ids = []\n",
    "            \n",
    "            previous_word_idx = None\n",
    "            for word_idx in word_ids:\n",
    "                if word_idx is None:\n",
    "                    label_ids.append(-100)\n",
    "                elif word_idx != previous_word_idx:\n",
    "                    if word_idx < len(example['labels']):\n",
    "                        label = example['labels'][word_idx]\n",
    "                        label_ids.append(label2id.get(label, label2id['O']))\n",
    "                    else:\n",
    "                        label_ids.append(label2id['O'])\n",
    "                else:\n",
    "                    label_ids.append(-100)\n",
    "                \n",
    "                previous_word_idx = word_idx\n",
    "            \n",
    "            labels.append(label_ids)\n",
    "        \n",
    "        tokenized_inputs[\"labels\"] = labels\n",
    "        return tokenized_inputs\n",
    "    \n",
    "    dataset_dict = {\n",
    "        'input_ids': [],\n",
    "        'attention_mask': [],\n",
    "        'labels': []\n",
    "    }\n",
    "    \n",
    "    batch_size = 100\n",
    "    for i in range(0, len(examples), batch_size):\n",
    "        batch = examples[i:i + batch_size]\n",
    "        tokenized = tokenize_and_align_labels(batch)\n",
    "        \n",
    "        dataset_dict['input_ids'].extend(tokenized['input_ids'].tolist())\n",
    "        dataset_dict['attention_mask'].extend(tokenized['attention_mask'].tolist())\n",
    "        dataset_dict['labels'].extend(tokenized['labels'])\n",
    "    \n",
    "    return Dataset.from_dict(dataset_dict)\n",
    "\n",
    "train_examples, val_examples, test_examples = split_data(\n",
    "    examples, config.train_split, config.val_split, config.test_split\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "\n",
    "print(\"Creating datasets...\")\n",
    "train_dataset = create_dataset(train_examples, tokenizer, label2id, config.max_length)\n",
    "val_dataset = create_dataset(val_examples, tokenizer, label2id, config.max_length)\n",
    "test_dataset = create_dataset(test_examples, tokenizer, label2id, config.max_length)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T20:11:10.047708Z",
     "iopub.status.busy": "2025-09-02T20:11:10.047440Z",
     "iopub.status.idle": "2025-09-02T20:11:12.372791Z",
     "shell.execute_reply": "2025-09-02T20:11:12.372103Z",
     "shell.execute_reply.started": "2025-09-02T20:11:10.047677Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def initialize_model(model_name: str, num_labels: int, label2id: Dict[str, int], id2label: Dict[int, str]):\n",
    "    \"\"\"Initialize the token classification model.\"\"\"\n",
    "    \n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=num_labels,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Initialized model: {model_name}\")\n",
    "    logger.info(f\"Number of parameters: {model.num_parameters():,}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = initialize_model(\n",
    "    config.model_name,\n",
    "    len(label2id),\n",
    "    label2id,\n",
    "    id2label\n",
    ")\n",
    "\n",
    "print(f\"Model initialized with {len(label2id)} labels\")\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T20:11:41.949682Z",
     "iopub.status.busy": "2025-09-02T20:11:41.949401Z",
     "iopub.status.idle": "2025-09-02T20:11:41.992598Z",
     "shell.execute_reply": "2025-09-02T20:11:41.991810Z",
     "shell.execute_reply.started": "2025-09-02T20:11:41.949660Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute metrics for evaluation.\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    \n",
    "    true_predictions = [\n",
    "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [id2label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
    "    flat_predictions = [item for sublist in true_predictions for item in sublist]\n",
    "    \n",
    "    report = classification_report(\n",
    "        flat_true_labels, \n",
    "        flat_predictions, \n",
    "        output_dict=True,\n",
    "        zero_division=0\n",
    "    )\n",
    "    \n",
    "    metrics = {\n",
    "        'precision': report['macro avg']['precision'],\n",
    "        'recall': report['macro avg']['recall'],\n",
    "        'f1': report['macro avg']['f1-score'],\n",
    "        'accuracy': report['accuracy']\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{OUTPUT_PATH}/bert_pii_model\",\n",
    "    num_train_epochs=config.num_epochs,\n",
    "    per_device_train_batch_size=config.batch_size,\n",
    "    per_device_eval_batch_size=config.batch_size,\n",
    "    warmup_steps=config.warmup_steps,\n",
    "    weight_decay=config.weight_decay,\n",
    "    learning_rate=config.learning_rate,\n",
    "    logging_dir=f\"{OUTPUT_PATH}/logs\",\n",
    "    logging_steps=50,\n",
    "    logging_first_step=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=2,\n",
    "    report_to=[],\n",
    "    dataloader_pin_memory=False,\n",
    "    disable_tqdm=False,\n",
    "    log_level=\"info\",\n",
    "    logging_nan_inf_filter=False,\n",
    "    log_on_each_node=False,\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured:\")\n",
    "print(f\"  Epochs: {config.num_epochs}\")\n",
    "print(f\"  Batch size: {config.batch_size}\")\n",
    "print(f\"  Learning rate: {config.learning_rate}\")\n",
    "print(f\"  Output directory: {training_args.output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T20:11:56.405085Z",
     "iopub.status.busy": "2025-09-02T20:11:56.404360Z",
     "iopub.status.idle": "2025-09-02T20:11:56.416263Z",
     "shell.execute_reply": "2025-09-02T20:11:56.415479Z",
     "shell.execute_reply.started": "2025-09-02T20:11:56.405062Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ProgressCallback(TrainerCallback):\n",
    "    \"\"\"Custom callback to display training progress.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.training_start_time = None\n",
    "        self.last_log_time = None\n",
    "        self.in_kaggle = 'KAGGLE_WORKING_DIR' in os.environ\n",
    "        \n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        import time\n",
    "        self.training_start_time = time.time()\n",
    "        print(\"=\" * 80)\n",
    "        print(\"TRAINING STARTED!\")\n",
    "        print(\"=\" * 80)\n",
    "        logger.info(f\"Total steps: {state.max_steps}\")\n",
    "        logger.info(f\"üìàotal epochs: {args.num_train_epochs}\")\n",
    "        logger.info(f\"Batch size: {args.per_device_train_batch_size}\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "    def on_epoch_begin(self, args, state, control, **kwargs):\n",
    "        epoch_num = int(state.epoch) + 1\n",
    "        print(f\"\\nEPOCH {epoch_num}/{args.num_train_epochs} STARTED\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs:\n",
    "            step = state.global_step\n",
    "            max_steps = state.max_steps\n",
    "            progress = (step / max_steps) * 100 if max_steps > 0 else 0\n",
    "            \n",
    "            bar_length = 40\n",
    "            filled_length = int(bar_length * step // max_steps) if max_steps > 0 else 0\n",
    "            bar = '‚ñà' * filled_length + '‚ñë' * (bar_length - filled_length)\n",
    "            \n",
    "            if self.in_kaggle and step % (args.logging_steps * 2) == 0:\n",
    "                try:\n",
    "                    from IPython.display import clear_output\n",
    "                    clear_output(wait=True)\n",
    "                    print(\"BERT PII Training Progress\")\n",
    "                    print(\"=\" * 50)\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            print(f\"\\nStep {step:4d}/{max_steps} [{bar}] {progress:.1f}%\")\n",
    "            \n",
    "            if \"loss\" in logs:\n",
    "                print(f\"   Loss: {logs['loss']:.4f}\")\n",
    "            if \"learning_rate\" in logs:\n",
    "                print(f\"   LR: {logs['learning_rate']:.2e}\")\n",
    "                \n",
    "            if \"eval_loss\" in logs:\n",
    "                print(f\"   Eval Loss: {logs['eval_loss']:.4f}\")\n",
    "            if \"eval_f1\" in logs:\n",
    "                print(f\"   F1 Score: {logs['eval_f1']:.4f}\")\n",
    "            if \"eval_precision\" in logs:\n",
    "                print(f\"   Precision: {logs['eval_precision']:.4f}\")\n",
    "            if \"eval_recall\" in logs:\n",
    "                print(f\"   Recall: {logs['eval_recall']:.4f}\")\n",
    "            \n",
    "            if self.in_kaggle:\n",
    "                import time\n",
    "                current_time = time.strftime(\"%H:%M:%S\")\n",
    "                print(f\"   ‚è∞ Time: {current_time}\")\n",
    "                \n",
    "            import sys\n",
    "            sys.stdout.flush()\n",
    "                \n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        print(f\"\\nüìä Evaluation completed at step {state.global_step}\")\n",
    "        \n",
    "    def on_save(self, args, state, control, **kwargs):\n",
    "        print(f\"üíæ Model checkpoint saved at step {state.global_step}\")\n",
    "        \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        import time\n",
    "        if self.training_start_time:\n",
    "            total_time = time.time() - self.training_start_time\n",
    "            hours = int(total_time // 3600)\n",
    "            minutes = int((total_time % 3600) // 60)\n",
    "            seconds = int(total_time % 60)\n",
    "            print(\"\\n\" + \"=\" * 80)\n",
    "            print(\"TRAINING COMPLETED!\")\n",
    "            print(f\"Total training time: {hours:02d}:{minutes:02d}:{seconds:02d}\")\n",
    "            print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T20:12:59.478337Z",
     "iopub.status.busy": "2025-09-02T20:12:59.477570Z",
     "iopub.status.idle": "2025-09-02T20:56:20.980502Z",
     "shell.execute_reply": "2025-09-02T20:56:20.979920Z",
     "shell.execute_reply.started": "2025-09-02T20:12:59.478305Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[ProgressCallback()],\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized. Starting training...\")\n",
    "\n",
    "print(\"\\nTesting display capabilities...\")\n",
    "print(\"  Basic print works\")\n",
    "logger.info(\"   Logger works\")\n",
    "\n",
    "if True:\n",
    "    try:\n",
    "        from IPython.display import clear_output, display\n",
    "        print(\"   IPython display available\")\n",
    "    except ImportError:\n",
    "        print(\"   IPython display not available\")\n",
    "\n",
    "print(\"   Progress bar test:\")\n",
    "test_bar = '‚ñà' * 10 + '‚ñë' * 30\n",
    "print(f\"      [{test_bar}] 25.0%\")\n",
    "\n",
    "print(\"\\nTraining configuration:\")\n",
    "print(f\"   Dataset size: {len(train_dataset)} train, {len(val_dataset)} validation\")\n",
    "print(f\"   Batch size: {config.batch_size}\")\n",
    "print(f\"   Learning rate: {config.learning_rate}\")\n",
    "print(f\"   Epochs: {config.num_epochs}\")\n",
    "print(f\"   Logging every {training_args.logging_steps} steps\")\n",
    "print(f\"   Evaluation every {training_args.eval_steps} steps\")\n",
    "print(f\"   Environment: {'Kaggle Notebook' if True else 'Local'}\")\n",
    "print(f\"   Native tqdm: {'Disabled' if training_args.disable_tqdm else 'Enabled'}\")\n",
    "\n",
    "try:\n",
    "    training_result = trainer.train()\n",
    "    logger.info(\"Training completed successfully!\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Training failed: {e}\")\n",
    "    raise\n",
    "\n",
    "print(\"Training completed!\")\n",
    "print(f\"Training loss: {training_result.training_loss:.4f}\")\n",
    "\n",
    "if config.save_model:\n",
    "    trainer.save_model(f\"{OUTPUT_PATH}/final_model\")\n",
    "    tokenizer.save_pretrained(f\"{OUTPUT_PATH}/final_model\")\n",
    "    print(f\"Model saved to {OUTPUT_PATH}/final_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-02T20:59:10.161245Z",
     "iopub.status.busy": "2025-09-02T20:59:10.160989Z",
     "iopub.status.idle": "2025-09-02T20:59:38.620324Z",
     "shell.execute_reply": "2025-09-02T20:59:38.619764Z",
     "shell.execute_reply.started": "2025-09-02T20:59:10.161225Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if config.evaluate_on_test:\n",
    "    print(\"Evaluating on test set...\")\n",
    "    \n",
    "    test_results = trainer.evaluate(test_dataset)\n",
    "    \n",
    "    print(\"Test Results:\")\n",
    "    for key, value in test_results.items():\n",
    "        if key.startswith('eval_'):\n",
    "            print(f\"  {key[5:]}: {value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8191537,
     "sourceId": 12944297,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
