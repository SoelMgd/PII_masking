# üéØ Guide Kaggle : Fine-tuning Mistral pour Classification de Tokens PII

Ce guide explique comment utiliser les scripts de classification par token sur Kaggle pour fine-tuner un mod√®le Mistral avec une approche "BERT-like".

## üìã Vue d'ensemble

Cette approche diff√®re du fine-tuning API en :
- **Gel du backbone Mistral** : Seule la t√™te de classification est entra√Æn√©e
- **Classification par token** : Chaque token re√ßoit un label PII (comme BERT NER)
- **Co√ªt r√©duit** : Moins de param√®tres √† entra√Æner
- **Contr√¥le total** : Entra√Ænement local avec PyTorch

## üõ†Ô∏è √âtape 1 : Pr√©paration des Donn√©es (Local)

### G√©n√©ration du dataset

```bash
# Depuis le r√©pertoire experiments/token_classification/
uv run dataset_processing.py --data-dir ../../data --max-english 1000 --max-french 1000
```

### Fichiers g√©n√©r√©s
- `../../data/token_classification/train_dataset.pkl` : Dataset d'entra√Ænement
- `../../data/token_classification/val_dataset.pkl` : Dataset de validation  
- `../../data/token_classification/label_mappings.json` : Mapping des labels
- `../../data/token_classification/dataset_stats.json` : Statistiques

### Upload vers Kaggle
1. Cr√©er un dataset Kaggle avec ces fichiers
2. Ou utiliser l'API Kaggle pour upload automatique

## üöÄ √âtape 2 : Fine-tuning sur Kaggle

### Configuration Kaggle requise
- **GPU** : P100 ou T4 minimum (16GB VRAM recommand√©)
- **Internet** : Activ√© pour t√©l√©charger le mod√®le Mistral
- **Dur√©e** : 2-4 heures selon la taille du dataset

### Script d'entra√Ænement

```python
# Dans un notebook Kaggle
!pip install transformers torch accelerate sentencepiece

# Copier le contenu de kaggle_finetuning.py
# Puis ex√©cuter :

python kaggle_finetuning.py \
    --train-dataset /kaggle/input/pii-token-data/train_dataset.pkl \
    --val-dataset /kaggle/input/pii-token-data/val_dataset.pkl \
    --output-dir ./mistral_token_classifier \
    --model-name mistralai/Mistral-7B-v0.1 \
    --batch-size 4 \
    --learning-rate 2e-5 \
    --num-epochs 3 \
    --max-length 512
```

### Param√®tres recommand√©s

| Param√®tre | Valeur | Description |
|-----------|--------|-------------|
| `batch_size` | 4-8 | Selon la VRAM disponible |
| `learning_rate` | 2e-5 | Taux d'apprentissage pour la t√™te |
| `num_epochs` | 3-5 | Nombre d'√©poques |
| `max_length` | 512 | Longueur max des s√©quences |
| `gradient_accumulation_steps` | 4 | Pour simuler des batch plus grands |

### Monitoring de l'entra√Ænement

Le script affiche :
- **Loss par epoch** : Doit diminuer progressivement
- **F1-Score validation** : M√©trique principale √† optimiser
- **Param√®tres entra√Ænables** : Seulement ~1% du mod√®le total

## üìä √âtape 3 : √âvaluation sur Kaggle

### Script d'√©valuation

```python
# Apr√®s l'entra√Ænement, √©valuer le mod√®le
python eval.py \
    --model-dir ./mistral_token_classifier \
    --test-dataset /kaggle/input/pii-token-data/val_dataset.pkl \
    --output-file evaluation_results.json \
    --batch-size 16
```

### M√©triques calcul√©es
- **F1-Score** (Weighted, Macro, Micro)
- **Pr√©cision/Rappel par classe**
- **Matrice de confusion**
- **Analyse d'erreurs d√©taill√©e**

## üîß Structure des Scripts

### `dataset_processing.py`
```python
# Fonctionnalit√©s principales :
- Tokenisation avec Mistral tokenizer
- Alignement des labels PII avec les tokens
- Cr√©ation de datasets PyTorch compatibles
- Gestion multilingue (anglais/fran√ßais)
```

### `kaggle_finetuning.py`
```python
# Architecture du mod√®le :
class MistralTokenClassifier(nn.Module):
    def __init__(self, model_name, num_labels):
        self.backbone = AutoModel.from_pretrained(model_name)  # Gel√©
        self.classifier = nn.Linear(hidden_size, num_labels)   # Entra√Ænable
```

### `eval.py`
```python
# √âvaluation compl√®te :
- Chargement du mod√®le entra√Æn√©
- Pr√©dictions sur dataset de test
- Calcul de m√©triques d√©taill√©es
- Analyse d'erreurs et exemples
```

## üí° Optimisations Kaggle

### Gestion m√©moire
```python
# Dans kaggle_finetuning.py
torch_dtype=torch.float16,  # Pr√©cision r√©duite
device_map="auto",          # Distribution automatique
gradient_accumulation_steps=4  # Batch virtuel plus grand
```

### Acc√©l√©ration
```python
# Utiliser gradient checkpointing si n√©cessaire
model.gradient_checkpointing_enable()

# Optimiser le DataLoader
num_workers=2,  # Parall√©lisation
pin_memory=True  # Transfert GPU plus rapide
```

## üìà R√©sultats Attendus

### Performance cible
- **F1-Score** : 0.75-0.85 (selon la taille du dataset)
- **Temps d'entra√Ænement** : 2-4 heures sur T4
- **M√©moire utilis√©e** : ~12-14GB VRAM

### Comparaison avec les approches
| Approche | F1-Score | Co√ªt | Temps |
|----------|----------|------|-------|
| API Fine-tuning | 0.87 | $15-25 | 30min |
| Token Classification | 0.75-0.85 | Gratuit | 2-4h |
| Few-shot Baseline | 0.67 | $2-5 | 10min |

## üêõ D√©pannage Kaggle

### Erreurs communes

1. **CUDA Out of Memory**
   ```python
   # R√©duire batch_size √† 2 ou 4
   # Augmenter gradient_accumulation_steps
   # Utiliser torch.float16
   ```

2. **Mod√®le trop lent √† charger**
   ```python
   # Utiliser un mod√®le plus petit
   model_name = "mistralai/Mistral-7B-v0.1"  # Au lieu de Mistral-8x7B
   ```

3. **Alignement des tokens**
   ```python
   # Le script utilise un fallback robuste
   # V√©rifier les logs pour les warnings
   ```

### Optimisations de performance
```python
# Dans le notebook Kaggle
import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"  # √âviter les warnings
torch.backends.cudnn.benchmark = True  # Optimiser CUDNN
```

## üìù Exemple de Notebook Kaggle

```python
# Cellule 1: Installation
!pip install transformers torch accelerate sentencepiece scikit-learn tqdm

# Cellule 2: Imports et configuration
import torch
import json
from pathlib import Path

# V√©rifier GPU
print(f"GPU disponible: {torch.cuda.is_available()}")
print(f"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}")

# Cellule 3: Copier les scripts
# Copier le contenu de kaggle_finetuning.py et eval.py

# Cellule 4: Entra√Ænement
# Ex√©cuter le script d'entra√Ænement

# Cellule 5: √âvaluation
# Ex√©cuter le script d'√©valuation

# Cellule 6: Sauvegarde des r√©sultats
# Sauvegarder le mod√®le et les r√©sultats
```

## üéØ Conseils pour Optimiser les R√©sultats

### Hyperparam√®tres
- **Learning Rate** : Commencer par 2e-5, r√©duire si instable
- **Batch Size** : Maximiser selon la VRAM disponible
- **Epochs** : 3-5 epochs suffisent g√©n√©ralement

### Donn√©es
- **√âquilibrage** : M√©langer anglais/fran√ßais 50/50
- **Taille** : 1000+ exemples par langue recommand√©
- **Qualit√©** : V√©rifier l'alignement des labels

### Architecture
- **Dropout** : 0.1 par d√©faut, augmenter si overfitting
- **Gel** : Garder le backbone gel√© pour la stabilit√©
- **Initialisation** : Utiliser l'initialisation par d√©faut

---

**Note** : Ce guide assume l'utilisation de Kaggle avec GPU. Pour CPU uniquement, r√©duire drastiquement la taille du mod√®le et du dataset. 